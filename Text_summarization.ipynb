{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "fmPZcHmfNuku"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.cluster.util import cosine_distance\n",
        "import numpy as np\n",
        "import networkx as nx\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pdfminer.six\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lrTmofJWPwJY",
        "outputId": "175a635c-9017-451d-dbd0-18623ec6532c"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pdfminer.six\n",
            "  Downloading pdfminer.six-20231228-py3-none-any.whl (5.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m43.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six) (3.3.2)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six) (42.0.5)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=36.0.0->pdfminer.six) (1.16.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six) (2.21)\n",
            "Installing collected packages: pdfminer.six\n",
            "Successfully installed pdfminer.six-20231228\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from pdfminer.high_level import extract_text\n",
        "\n",
        "text = extract_text(\"leave-policy.pdf\")\n",
        "\n",
        "# Save the text to a file\n",
        "with open(\"leave_policy_text.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "  f.write(text)\n"
      ],
      "metadata": {
        "id": "xRPEfRoJPakX"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text(text):\n",
        "    # Remove numbers\n",
        "    text = re.sub(r'\\d+', '', text)\n",
        "\n",
        "    # Remove special characters and single-word lines\n",
        "    lines = text.split('\\n')\n",
        "    cleaned_lines = []\n",
        "    for line in lines:\n",
        "        line = re.sub(r'[^\\w\\s]', '', line)  # Remove special characters\n",
        "        if len(line.strip().split()) > 5:    # Keep lines with more than one word\n",
        "            cleaned_lines.append(line.strip())\n",
        "\n",
        "    # Remove empty lines\n",
        "    cleaned_text = '\\n'.join(filter(None, cleaned_lines))\n",
        "\n",
        "    return cleaned_text\n",
        "\n",
        "# Read the text from the file\n",
        "with open(\"leave_policy_text.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    text = f.read()\n",
        "\n",
        "# Clean the text\n",
        "cleaned_text = clean_text(text)\n",
        "\n",
        "# Save the cleaned text to a file\n",
        "with open(\"cleaned_leave_policy_text.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(cleaned_text)\n"
      ],
      "metadata": {
        "id": "DLqIcLZKRmCK"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import networkx as nx\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.cluster.util import cosine_distance\n",
        "import numpy as np\n",
        "import re\n",
        "\n",
        "nltk.download(\"punkt\")\n",
        "\n",
        "def read_article(file_name):\n",
        "    file = open(file_name, \"r\")\n",
        "    filedata = file.read()\n",
        "    file.close()\n",
        "    return filedata\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Owd-i9u4XfbW",
        "outputId": "6467453c-ff0b-4f58-d7a3-38968369cdcb"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Summarized text has been written to /content/summarized_output.txt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def sentence_similarity(sent1, sent2, stopwords=None):\n",
        "    if stopwords is None:\n",
        "        stopwords = []\n",
        "\n",
        "    sent1 = [w.lower() for w in sent1]\n",
        "    sent2 = [w.lower() for w in sent2]\n",
        "\n",
        "    all_words = list(set(sent1 + sent2))\n",
        "\n",
        "    vector1 = [0] * len(all_words)\n",
        "    vector2 = [0] * len(all_words)\n",
        "\n",
        "    # build the vector for the first sentence\n",
        "    for w in sent1:\n",
        "        if w in stopwords:\n",
        "            continue\n",
        "        vector1[all_words.index(w)] += 1\n",
        "\n",
        "    # build the vector for the second sentence\n",
        "    for w in sent2:\n",
        "        if w in stopwords:\n",
        "            continue\n",
        "        vector2[all_words.index(w)] += 1\n",
        "\n",
        "    return 1 - cosine_distance(vector1, vector2)\n",
        "\n"
      ],
      "metadata": {
        "id": "gWIGTwP7ccqi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_similarity_matrix(sentences, stop_words):\n",
        "    # Create an empty similarity matrix\n",
        "    similarity_matrix = np.zeros((len(sentences), len(sentences)))\n",
        "\n",
        "    for idx1 in range(len(sentences)):\n",
        "        for idx2 in range(len(sentences)):\n",
        "            if idx1 == idx2: # Ignore if both are same sentences\n",
        "                continue\n",
        "            similarity_matrix[idx1][idx2] = sentence_similarity(sentences[idx1], sentences[idx2], stop_words)\n",
        "\n",
        "    return similarity_matrix\n",
        "\n"
      ],
      "metadata": {
        "id": "-w3RBTHXce1l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_summary(file_name, output_file, top_n=50):\n",
        "    stop_words = stopwords.words('english')\n",
        "    summarize_text = []\n",
        "\n",
        "    # Step 1 - Read text and split it into sentences\n",
        "    article = read_article(file_name)\n",
        "    sentences = nltk.sent_tokenize(article)\n",
        "\n",
        "    # Step 2 - Generate Similarity Matrix across sentences\n",
        "    sentence_similarity_matrix = build_similarity_matrix(sentences, stop_words)\n",
        "\n",
        "    # Step 3 - Rank sentences in similarity matrix\n",
        "    sentence_similarity_graph = nx.from_numpy_array(sentence_similarity_matrix)\n",
        "    scores = nx.pagerank(sentence_similarity_graph)\n",
        "\n",
        "    # Step 4 - Sort the rank and pick top sentences\n",
        "    ranked_sentence = sorted(((scores[i],s) for i,s in enumerate(sentences)), reverse=True)\n",
        "    if top_n > len(ranked_sentence):\n",
        "        top_n = len(ranked_sentence)\n",
        "\n",
        "    for i in range(top_n):\n",
        "        summarize_text.append(\"\".join(ranked_sentence[i][1]))\n",
        "\n",
        "    # Step 5 - Write the summarized text to the output file\n",
        "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(\". \".join(summarize_text))\n",
        "\n",
        "    print(f\"Summarized text has been written to {output_file}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "DqcYoza0cg6d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "generate_summary(\"/content/cleaned_leave_policy_text.txt\", \"/content/summarized_output.txt\", 5)"
      ],
      "metadata": {
        "id": "4k-Gnt6Hci3w"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}